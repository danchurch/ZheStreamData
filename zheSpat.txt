
## the goal here is to see if we can uncover spatial
## signatures in Zhe's nitrogen and sulfur metabolism data

mv 'Normalized_For Dan.xlsx' 'Normalized_For_Dan.xlsx'
mv 'water chemistry.xlsx' 'waterChemistry.xlsx'

## read it into a df

import pandas as pd
import scipy.stats as ss
import matplotlib.pyplot as plt; plt.ion()
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns
from sklearn.linear_model import LinearRegression
from scipy import stats


#enz = pd.read_excel("Normalized_OnlyEnzymes.xlsx")
enz = pd.read_csv("Normalized_OnlyEnzymes.csv")
enz.rename(columns = {'Unnamed: 0':'enzyme'}, inplace='True')
enz['enzyme'] = enz['enzyme'].str.replace(',','')
enz.set_index('enzyme', inplace=True)
enz.head()

## hmm, a couple concerns... 
## need to cut off environmental data (first several rows) 
## first we need split this by nitrogen or sulphur, other metabolism
## related enzymes, also by depth class, 

## first step would be univariate linear models for each
## but we are expecting a drop in the central region of 
## so not linear by some definitions. Something like 
## a negative and positive quadratic. 

## Zhe just sent me an updated spreadsheet:
waterChemistry = pd.read_excel("waterChemistry.xlsx")
## so our spreadsheet is now very simple.
waterChemistry = waterChemistry[['Section', 'NO3', 'SO4']]

waterChemistry

## according to our 

## focus for a minute on nitrogen metabolism
## use depth = 5

enzEnv = pd.read_excel('enzymeEnv.xlsx')
enzEnv.rename(columns = {'Unnamed: 0':'position'}, inplace='True')
enzEnv.set_index('position', inplace=True)
enzEnv.rename(index = {'Distance(m)(all distance to the upstream)':'Distance',
                       'Depth(m)':'Depth'}, 
                       inplace='True')
enzEnv.head()

## Nenz = enz[enz.Function == "N metabolism"] ## better definition below


##Nshallow = enz[enz.Function == "N metabolism"][['Mid-A_5','Mid-B_5','Down_5','KU_5']]

##enzEnvShallow = enzEnv[['Mid-A_5','Mid-B_5','Down_5','KU_5']]

enzEnvShallow

Nshallow

waterChemistry

## we only have nitrate data for 

## nitrogen levels for this:

waterChemistry

Nshallow

## we want to test a correlation with stream Nitrate values
## we expect nitrification rates to increase with 

## we are focusing on the shallow depth (5 cm), so we have to 
## throw out our upper stream sample site. 
## and comparing to the confluence seems odd to me. 
## so we are down to mid- and down- stream sections. 

## that doesn't leave us much to work with....

## Seems appropriate to use the mean our two 

## also, not sure if amoA is of interest here.
## we have almost none-existent ammonia levels
## not sure if amoA

## so drop Amo, average the mid columns, and the confluence site:

 
Nenz = (enz[enz.Function == "N metabolism"][["Mid-A_5","Mid-B_5","Down_5"]]
        .drop(axis='rows', labels='Ammonia monooxygenase (amoA)'))

## our mean gene count for the mid site is:

N_Mid_5 = Nenz[['Mid-A_5','Mid-B_5']].mean(axis='columns')

## a new, greatly simplified df:

nRedu = pd.concat([N_Mid_5, Nenz['Down_5']], axis='columns') 
nRedu.rename(columns = {0:'mid', 'Down_5':'down'}, inplace='True')
nRedu

## so every enzyme in the denitrification pathway shows lower counts
## downstream?

ss.ttest_rel(nRedu['mid'], nRedu['down'])

## check in with Zhe, make sure these are all such genes (denitrifying)


## repeat for sulfur. 
## for sulfur oxidizing enzymes: we expect an increase
## in sulfur oxidation, higher gene counts.
## let's see: 

Senz = enz[enz.Function == "S metabolism"][["Mid-A_5","Mid-B_5","Down_5"]]
S_Mid_5 = Senz[['Mid-A_5','Mid-B_5']].mean(axis='columns')
sRedu = pd.concat([S_Mid_5, Senz['Down_5']], axis='columns') 
sRedu.rename(columns = {0:'mid', 'Down_5':'down'}, inplace='True')
sRedu

## yeah, that is not clear. I need to talk to Zhe about his expectations
## fore these genes. I don't feel like reading about each of them. 

## involve all samples, including upstream,
## = all depth
## break out the midstream samples
## model all by distance along 



############

## where were we? 

## more work on the nitrification pathway - do an RDA of of de-nitrification 
## genes as a function of distance downstream 

## run this for both depths. 

## which means we need to parse out two matrices, one for each depth. 

## also, how different are the two depths of the stream? 

## run an nms, permanova, etc

## then start thinking about sulfur.

## step 1, separate out depths.

## for the 15 cm samples:

deepSamples = enzEnv.loc["Depth"][enzEnv.loc["Depth"] == 0.15].index.to_list()

shallowSamples = enzEnv.loc["Depth"][enzEnv.loc["Depth"] == 0.05].index.to_list()[:-1]

deepNenz = (enz[enz.Function == "N metabolism"][deepSamples]
        .drop(axis='rows', labels='Ammonia monooxygenase (amoA)'))
shallowNenz = (enz[enz.Function == "N metabolism"][shallowSamples]
        .drop(axis='rows', labels='Ammonia monooxygenase (amoA)'))
## let's just keep the abbreviations:
aa = (deepNenz.reset_index()['enzyme']
        .str.split(pat="(", n=1, expand=True)[1]
        .str.replace(")","")
    )
aa.name = "enzyme"
deepNenz.index = aa
shallowNenz.index = aa

## visualize 
deepDist = enzEnv[deepSamples].loc['Distance']
deepNenzT = deepNenz.transpose()
deepNenzT.index = deepDist
deepNenzT


fig1, ax1 = plt.subplots()
deepNenzT.plot.line(title='15cm N-reducing gene counts', ax=ax1)

help(deepNenzT.plot.line)

## jeezus this is frustrating, we need that missing 
## nitrogen measurement. 

## all the potential denitrification enzymes 
## show the exact same spatial pattern, 
## but we lack the solute level data

## repeat with the shallow samples, how do they look?:

fig2, ax2 = plt.subplots()

shallowDist = enzEnv[shallowSamples].loc['Distance']

shallowNenz
shallowNenzT = shallowNenz.transpose()
shallowNenzT.index = shallowDist
shallowNenzT.plot.line(title='5cm N-reducing gene counts', ax=ax2)

ax2.set_xlim(ax1.get_xlim())

## let's put these together:

fig, ax = plt.subplots(2,1, sharex=True)
deepNenzT.plot.line(title='15cm N-reducing gene counts', ax=ax[0], legend=False)
shallowNenzT.plot.line(title='5cm N-reducing gene counts', ax=ax[1])

plt.close('all')

## great

## so, now what? try an RDA for both of these

deepNenzT.to_csv("deepNenz.csv")
shallowNenzT.to_csv("shallowNenz.csv")

deepNenz

## for that we need genes as columns, sites as rows?
## same with the spatial data? 

### R ###

library("vegan")

## let's try a simple RDA of the gene abundances 
## against distance down-stream

deepNenz <- read.csv('deepNenz.csv')
shallowNenz <- read.csv('shallowNenz.csv')

## first let's transform the gene abundances to 
## approximately meet linear/euclidean requirenments
deepNenz.hell <- decostand(deepNenz[-1], 'hellinger')
shallowNenz.hell <- decostand(shallowNenz[-1], 'hellinger')
distance <- deepNenz[[1]]
deepNenz.hell.rda <- rda(deepNenz.hell ~ distance)
plot(deepNenz.hell.rda)
## points to the only obvious global trend, 
## the increase in nar operon with distance

## and also the sulphur? I doubt the rda function
## is built to accomodate any non-normal residuals,
## and this is also count data, so...

deepSenz.hell <- decostand(deepSenz[-1], 'hellinger')
distance <- deepSenz[[1]]
deepSenz.hell.rda <- rda(deepSenz.hell ~ distance)
shallowSenz.hell <- decostand(shallowSenz[-1], 'hellinger')
distance <- shallowSenz[[1]]
shallowSenz.hell.rda <- rda(shallowSenz.hell ~ distance)

plot(shallowSenz.hell.rda) ## no strong distance dependency in the sulfur, not a surprise


#$png(file = 'deepDenitrificationRDA.png', height=600, width=1200)
#plot(deepNenz.hell.rda)
#dev.off()

## and without N-hellinger transformation
distance <- deepNenz[[1]]
deepNenz.rda <- rda(deepNenz[-1] ~ distance)
plot(deepNenz.rda)
## the transformed model appears more informative. 
## and this what legendre, borcard, etc, recommend

coef(deepNenz.hell.rda)


RsquareAdj(deepNenz.hell.rda)$adj.r.squared ## fairly large r2, but...

anova(deepNenz.hell.rda, permutations = how(nperm = 999)) ## p=0.1167
anova(deepNenz.hell.rda, by = "axis", permutations = how(nperm = 999)) ## p = 0.1667

## let's look at our sulfur matrix, that we created below?
deepSenz <- read.csv('deepSenz.csv')
shallowSenz <- read.csv('shallowSenz.csv')

## without the hellingar transformations of the sulphur data
#deepNenz.sulf.rda <- rda(deepNenz.hell, deepSenz)
#deepNenz.sulf.rda <- rda(deepNenz.hell ~  deepNenz$Distance)
#deepNenz.sulf.rda <- rda(deepNenz.hell ~ deepNenz$Distance + deepSenz$soxYZAXB + deepSenz$sat + deepSenz$sqr + deepSenz$soxC)
#deepNenz.sulf.rda <- rda(deepNenz.hell ~ deepSenz$soxYZAXB + deepSenz$sat + deepSenz$sqr + deepSenz$soxC + deepNenz$Distance)
deepNenz.sulf.rda <- rda(deepNenz.hell ~ distance + soxYZAXB, data=deepSenz)


## with the hellingar transformations of the sulphur data
## I think these are better, fits the assumptions of the rda.
deepNenz.sulf.rda <- rda(deepNenz.hell ~ distance + soxYZAXB, data=deepSenz.hell)

## does order matter here?
deepNenz.sulf.rda <- rda(deepNenz.hell ~ soxYZAXB + distance, data=deepSenz.hell)
## not really

anova.cca(deepNenz.sulf.rda)

#anova.cca(deepNenz.sulf.rda, permutations = how(nperm = 999)) 
## this doesn't return a pvalue after three explanatory terms, 
## too few degrees of freedom

anova.cca(deepNenz.sulf.rda, permutations = how(nperm = 999), by="term") ## same

?anova.cca

deepNenz.sulf.rda

summary(deepNenz.sulf.rda)

RsquareAdj(deepNenz.sulf.rda)$adj.r.squared 
## fairly large r2 (0.537), but not enough statistical power 
## to use as a model


png(file='NreductionRDA.png', width=1000, height=600)

par(mfrow=c(1,2))

par(mfrow=c(1,1))

#pdf(file='deepN_rda_soxAndDist.pdf')
## plot this, scale type 1
plot(deepNenz.sulf.rda,
    scaling = 1,
    display = c("sp", "lc", "cn"),
    main = "deepN RDA, sulfur and distance predictors")
spe.sc1 <-
    scores(deepNenz.sulf.rda,
        choices = 1:2,
        scaling = 1,
        display = "sp"
)
arrows(0, 0,
    spe.sc1[, 1] * 0.92,
    spe.sc1[, 2] * 0.92,
    length = 0,
    lty = 1,
    col = "red"
)
#dev.off()

## this is scaling type 1, basically look for projection 
## of the nitrogen genes onto the predictor axes
## borcard p217 for interpretation

## repeat this on the shallow sediments


shallowNenz

shallowSenz

#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance, data=shallowSenz) ## distance alone can't explain anything. Makes sense.
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + soxYZAXB, data=shallowSenz)

### all of the genes showing the triangle shape are collinears:

shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + soxYZAXB, data=shallowSenz)

#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + soxC, data=shallowSenz)
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ soxYZAXB + soxC, data=shallowSenz)
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ soxYZAXB + sat, data=shallowSenz)
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ soxYZAXB + dsrAB, data=shallowSenz)
###


#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + soxYZAXB + sat + sqr + soxC, data=shallowSenz)
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + soxC + sat + sqr + soxYZAXB, data=shallowSenz)
#shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ Distance + sat, data=shallowSenz)

## with the hellingar transformations of the sulphur data
shallowNenz.sulf.rda <- rda(shallowNenz.hell ~ distance + soxYZAXB, data=shallowSenz.hell)


shallowNenz.sulf.rda

vif.cca(shallowNenz.sulf.rda) ## meh, we are simply limited by our few samples. examined manually above

anova.cca(shallowNenz.sulf.rda)

## plot this, scale type 1

#par(mfrow=c(1,2))
#par(mfrow=c(1,1))
pdf(file='shallowN_rda_soxAndDist.pdf')
plot(shallowNenz.sulf.rda,
    scaling = 1,
    display = c("sp", "lc", "cn"),
    main = "shallowN RDA, sulfur and distance predictors")
spe.sc1 <-
    scores(shallowNenz.sulf.rda,
        choices = 1:2,
        scaling = 1,
        display = "sp"
)
arrows(0, 0,
    spe.sc1[, 1] * 0.92,
    spe.sc1[, 2] * 0.92,
    length = 0,
    lty = 1,
    col = "red"
)
dev.off()

## one interesting article that mentions this gene
## as part of a general membrane bound nitrate 
## reductase operon, in Pseudomonas aeruginosa
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3568144/

## and even more interesting:
https://journals.asm.org/doi/10.1128/JB.181.17.5303-5308.1999
## in this article, this operon is proposed to be a
## "high-substrate-induced" operon, in E. coli.
## (= low-affinity system?)
## as in, it is expressed when there is lots of 
## nitrate

## and so on. 

## how about a dbRDA:


deepNenz.cap <- capscale(deepNenz[-1] ~ deepNenz$Distance, dist="bray")

deepNenz.cap

plot(deepNenz.cap) ## similar as with hellinger transformed, but less informative
anova(deepNenz.cap) ## nope nope nope 




## taking a univariate approach
## can we say there is a statistically significant 
## increase with distance of this gene?
## let's go back to...

### python ###

## back to python:

deepNARcounts = deepNenzT['narGHIJK'].reset_index(drop=True)
deepDist = deepNenzT.reset_index()['Distance']

narGHIJK_model = sm.GLM(
                    deepNARcounts, 
                    deepDist, 
                    family=sm.families.NegativeBinomial())

narGHIJK_results = narGHIJK_model.fit()

print(narGHIJK_results.summary())

## which gives us:

                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               narGHIJK   No. Observations:                    4
Model:                            GLM   Df Residuals:                        3
Model Family:        NegativeBinomial   Df Model:                            0
Link Function:                    Log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -1396.2
Date:                Tue, 14 Jun 2022   Deviance:                       2722.8
Time:                        16:48:55   Pearson chi2:                 1.94e+06
No. Iterations:                    11   Pseudo R-squ. (CS):        -4.044e+295
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Distance       0.0223      0.001     15.627      0.000       0.020       0.025
==============================================================================

## hmm, I think I am reading that right, a very small p value. 

## just to make sure, use a function I am a little more familiar with:

deepNARcounts

deepDist

LinearRegression


X = deepDist.values.reshape(-1,1)
Y = deepNARcounts.values.reshape(-1,1)

reg = LinearRegression()

reg.fit(X,Y)

reg.predict(X)

reg.score()
## forgot, no frequentist pvalues for the sklearn people
## they can't be bothered with such dinosaurs

## so use scipy stats:

stats.linregress(deepDist, deepNARcounts) ## but can't fit negative binomial to this

## wonder what organism(s) this operon belongs to?


#### meeting todo list, after talking to Zhe:

## focus more on sulfur
## = rda of sulfur genes
## = look closely at dsrAB and soxYZAXB
## look for correlations between sulfur genes and NarG
## compare gene abundances by depth
## add confluence site to all charts
## model the change in slopes between depths at mid A and mid B 
## look for genes of interest in Sulfuricurvum?
## rerun the hellinger-transformed RDA of nitrogen as a dbRDA?


###### sulfur metabolism #######

## a good introduction is here:
https://doi.org/10.1111/j.1574-6976.2009.00187.x
## = Ghosh and Dam 2009

#### graph all sulfur genes

## probably a mess, but let's graph them all, and run an RDA on them
## might help us pickout genes to focus on


enzEnv

Senz = enz[enz.Function == "S metabolism"]


deepSamples

shallowSamples


## make sulphure gene matrix as above
deepSenz = (enz[enz.Function == "S metabolism"][deepSamples])
shallowSenz = (enz[enz.Function == "S metabolism"][shallowSamples])
aa = (deepSenz.reset_index()['enzyme']
        .str.split(pat="(", n=1, expand=True)[1]
        .str.replace(")","")
    )
aa.name = "enzyme"
deepSenz.index = aa
shallowSenz.index = aa
del(aa)
deepSenzT = deepSenz.transpose()
deepSenzT.index = deepDist
shallowSenzT = shallowSenz.transpose()
shallowSenzT.index = shallowDist
#deepSenzT.to_csv('deepSenz.csv')
#shallowSenzT.to_csv('shallowSenz.csv')

deepDist





fig1, ax1 = plt.subplots()
deepSenzT.plot.line(title='15cm S-metabolism gene counts', ax=ax1)

fig, ax = plt.subplots(2,1, sharex=True)
deepSenzT.plot.line(title='15cm S-metabolism gene counts', ax=ax[0], legend=False)
shallowSenzT.plot.line(title='5cm S-metabolism gene counts', ax=ax[1])

## interesting. We see the same shapes as with 
## the N-reducers in some of these enzymes. 

## I really hope Zhe did his normalizations right. 

## anyway, assuming he did...

## in the simplest scenario, we would expect sulfur
## oxidation in this system to be coupled with nitrate 
## reduction. So the congruency between some of these
## trends and the n-reducing genes is expected. 

## Zhe was especially interested in dsrAB and soxYZAXB
## soxYZAXB definitely shows a congruent trend as the
## the 

## according to wikipedia, dissimilatory sulfate reduction 
## is a respiration process with sulfate as the final 
## electron acceptor. 

## one would therefore expect activity of this enzyme this to be in competition 
## with reduction of nitrogen, no?

## a plot with just the two genes that Zhe wanted to look at:


plt.close('all')
shallowSenzT[['soxYZAXB', 'dsrAB']]
deepSenzT[['soxYZAXB', 'dsrAB']]
fig, ax = plt.subplots(2,1, sharex=True)
(deepSenzT[['soxYZAXB', 'dsrAB']]
    .plot.line(title='15cm S-metabolism gene counts', 
    ax=ax[0], legend=False))
(shallowSenzT[['soxYZAXB', 'dsrAB']]
    .plot.line(title='5cm S-metabolism gene counts', 
    ax=ax[1]))

## interesting, soxYZAXB behaves like we would expect.

## can we isolate just the genes that show a dramatic 
## difference between the two mid sites?

plt.close('all')
fig, ax = plt.subplots()
(shallowSenzT[['soxYZAXB', 'sat', 'sqr', 'soxC']]
    .plot.line(title='5cm S-metabolism gene counts', 
    ax=ax))
ax.set_xlim(0,550)
ax.set_ylim(0,3000)


## in the case of the 15cm depth, the genes that 
## show congruence with the n-reducers are:
## soxYZAXB', 'soxC', and 'sorAB'
## for 15cm depth, the genes that 
## show congruence with the n-reducers are:
## soxYZAXB', 'soxC', and 'sorAB'
## and for 5 cm:
## 'soxYZAXB', 'sat', 'sqr', 'soxC'

## we can also include dsrAB, a sulfur reducing 
## enzyme that Zhe was interested in, though: 
## 'dsrAB'

## that adds up to:

plt.close('all')
fig, ax = plt.subplots(2,1, sharex=True)
genes=['soxYZAXB', 'soxC', 'sat', 'sqr','dsrAB']
(deepSenzT[genes]
    .plot.line(title='15cm S-metabolism gene counts', 
    ax=ax[0], legend=False))
(shallowSenzT[genes]
    .plot.line(title='5cm S-metabolism gene counts', 
    ax=ax[1]))

## the two best examples of this are the promoter 'soxYZAXB'
## and the hydrogen sulfide ('sulfane') oxidizing enzyme 'soxC'

## sqr is some sort of sulfar-related quinone electron shuttle thing,
## I think. It also is probably bidirectional, as far as oxidation/reduction

## it is highly congruent in the deeper sediment, but not on the shallow
## sediments. 

## SAT is an enzyme for adenylating sulfate, and can be linked to both 
## oxidation and reducing pathways

#### correlations ####

## Zhe wanted to look for correlations between sulfur genes and NarG
## I'd say let's mash the sulfur and nitrogen genes together and make
## a correlation matrix. Again, broken up by depth.

## deep gene matrix, N + S  
## in python

deepSNcorr = (enz[(enz.Function == "S metabolism") | (enz.Function == "N metabolism")]
            [deepSamples]
            .transpose()
            .corr()
)

deepSNcorr ## looks okay

plt.close('all')
fig, ax = plt.subplots(figsize=(8,8))
im = ax.imshow(deepSNcorr)
ax.set_xticks(range(deepSNcorr.shape[0]))
ax.set_yticks(range(deepSNcorr.shape[0]))
ax.set_yticklabels(deepSNcorr.columns.to_list())
ax.set_xticklabels(deepSNcorr.columns.to_list(), rotation=90)
ax.set_title('S/N correlations, deep (15cm)', fontsize=20)
ax.vlines(x=8.5,
          ymin=-0.5,
          ymax=21.5,
          colors='black',
          linewidths=5
)
ax.hlines(y=8.5,
          xmax=-0.5,
          xmin=21.5,
          colors='black',
          linewidths=5
)
plt.tight_layout()

## need to tweak colormap. green = 0, blue = -1, yellow = 1
## good enough for now. 

## repeat for shallow:

shallowSNcorr = (enz[(enz.Function == "S metabolism") | (enz.Function == "N metabolism")]
            [shallowSamples]
            .transpose()
            .corr()
)


plt.close('all')
fig, ax = plt.subplots(figsize=(8,8))
im = ax.imshow(shallowSNcorr)
ax.set_xticks(range(shallowSNcorr.shape[0]))
ax.set_yticks(range(shallowSNcorr.shape[0]))
ax.set_yticklabels(shallowSNcorr.columns.to_list())
ax.set_xticklabels(shallowSNcorr.columns.to_list(), rotation=90)
ax.set_title('S/N correlations, shallow (5cm)', fontsize=20)
ax.vlines(x=8.5,
          ymin=-0.5,
          ymax=21.5,
          colors='black',
          linewidths=5
)
ax.hlines(y=8.5,
          xmax=-0.5,
          xmin=21.5,
          colors='black',
          linewidths=5
)
plt.tight_layout()


########## new homework ###########

## still need confluence data
## refine regression/correlation among N/S genes
## can we get a housekeeping gene like rpoB?
## explore Zhe's environmental data

##### confluence data ######
 
## let's repeat the above and get N and S gene matrices
## that include the confluence

## for N
deepSamples = enzEnv.loc["Depth"][enzEnv.loc["Depth"] == 0.15].index.to_list()
## that stays the same, we don't have confluence data for the deep
## this is different, we dropped confluence before:
shallowSamples = enzEnv.loc["Depth"][enzEnv.loc["Depth"] == 0.05].index.to_list()
deepDist = enzEnv[deepSamples].loc['Distance']
shallowDist = enzEnv[shallowSamples].loc['Distance']
deepNenz = (enz[enz.Function == "N metabolism"][deepSamples]
        .drop(axis='rows', labels='Ammonia monooxygenase (amoA)'))
shallowNenz = (enz[enz.Function == "N metabolism"][shallowSamples]
        .drop(axis='rows', labels='Ammonia monooxygenase (amoA)'))

## let's just keep the abbreviations:
aa = (deepNenz.reset_index()['enzyme']
        .str.split(pat="(", n=1, expand=True)[1]
        .str.replace(")","")
    )
aa.name = "enzyme"
deepNenz.index = aa
shallowNenz.index = aa

deepDist = enzEnv[deepSamples].loc['Distance']
deepNenzT = deepNenz.transpose()
deepNenzT.index = deepDist
deepNenzT

shallowDist = enzEnv[shallowSamples].loc['Distance']
shallowNenzT = shallowNenz.transpose()
shallowNenzT.index = shallowDist
shallowNenzT

## plotting N 
fig, ax = plt.subplots(2,1, sharex=True)
deepNenzT.plot.line(title='15cm N-reducing gene counts', ax=ax[0], legend=False)
shallowNenzT.plot.line(title='5cm N-reducing gene counts, including confluence', ax=ax[1])

## for Sulphur

## make sulphure gene matrix as above
deepSenz = (enz[enz.Function == "S metabolism"][deepSamples])
shallowSenz = (enz[enz.Function == "S metabolism"][shallowSamples])
aa = (deepSenz.reset_index()['enzyme']
        .str.split(pat="(", n=1, expand=True)[1]
        .str.replace(")","")
    )
aa.name = "enzyme"
deepSenz.index = aa
shallowSenz.index = aa
del(aa)
deepSenzT = deepSenz.transpose()
deepSenzT.index = deepDist
shallowSenzT = shallowSenz.transpose()
shallowSenzT.index = shallowDist

plt.close('all')

## plotting all sulphur related genes
fig, ax = plt.subplots(2,1, sharex=True)
deepSenzT.plot.line(title='15cm S-metabolism gene counts', ax=ax[0], legend=False)
shallowSenzT.plot.line(title='5cm S-metabolism gene counts, including confluence', ax=ax[1])

## subset to sulphur genes of interest:
fig, ax = plt.subplots(2,1, sharex=True)
genes=['soxYZAXB', 'soxC', 'sat', 'sqr','dsrAB']
(deepSenzT[genes]
    .plot.line(title='15cm S-metabolism gene counts', 
    ax=ax[0], legend=False))
(shallowSenzT[genes]
    .plot.line(title='5cm S-metabolism gene counts', 
    ax=ax[1]))

## subset to two unambiguous sulphur oxidizing genes:
fig, ax = plt.subplots(2,1, sharex=True)
genes=['soxYZAXB', 'soxC']
(deepSenzT[genes]
    .plot.line(title='15cm Sox gene counts', 
    ax=ax[0], legend=False))
(shallowSenzT[genes]
    .plot.line(title='5cm Sox gene counts', 
    ax=ax[1]))

## great, so adding the confluence made little difference

##### univariate regressions ####

## let's build some univariate models of nitrogen gene counts 
## as a function of sulfur oxidation genes (or vice versa)

## according to our RDAs, narGHIJK is most heavily, positively
## influenced by distance. 

## but I have trouble believing this entirely, looking at the 
## gene abundance charts. The same "lightning strike" shape 
## is visible in the deep (15cm) charts in both genes,
## and the same "upside-down check" in the shallow (5cm). 

## check with a good old fashioned regression, neg binom,
## as we did with distance above, @ "taking a univariate approach"

## repeat, with sox operon:

## back to python:

## with just the sox operon:
deepNARcounts = deepNenzT['narGHIJK'].reset_index(drop=True)
sox = deepSenzT['soxYZAXB'].reset_index(drop=True)
deep_narGHIJK_sox_model = sm.GLM(
                    deepNARcounts, 
                    sox, 
                    family=sm.families.NegativeBinomial())

deep_narGHIJK_sox_model_results = deep_narGHIJK_sox_model.fit()

print(deep_narGHIJK_sox_model_results.summary())

## multiple regression possible here? We just have 4 observations...

sox_deepDist = deepSenzT.reset_index()[['Distance','soxYZAXB']]
deepDist = deepNenzT.reset_index()['Distance']
sox = deepSenzT['soxYZAXB'].reset_index(drop=True)
deep_nar_by_soxDistance_model = sm.GLM(
                    deepNARcounts, 
                    sox_deepDist, 
                    family=sm.families.NegativeBinomial())

deep_nar_by_soxDistance_results = deep_nar_by_soxDistance_model.fit()

print(deep_nar_by_soxDistance_results.summary())

## both terms significant, but not much or any real coefficient, 
## I assume because the overall trends are almost flat, and 
## "slightly" opposite (slight positive vs. slight negative) 
## overall

## in general, somehow we need to focus our attention on the 
## real trend in the data, which are the slopes between 
## the two mid sites. I think this would be a comparison of 
## slopes 

## maybe get to that in a minute. 


## that is just not that useful. Why do so few regression
## functions include error families for count data? I must be
## missing something. 

## anyway, with shallow Nar operon

shallowNARcounts = shallowNenzT['narGHIJK'].reset_index(drop=True)
shallowDist = shallowNenzT.reset_index()['Distance']
sox = shallowSenzT['soxYZAXB'].reset_index(drop=True)
shallow_narGHIJK_sox_model = sm.GLM(
                    shallowNARcounts, 
                    sox, 
                    family=sm.families.NegativeBinomial())

shallow_narGHIJK_sox_model_results = shallow_narGHIJK_sox_model.fit()

print(shallow_narGHIJK_sox_model_results.summary())

## about the same. 

## well, at least it agrees with the RDA

## according to our RDA, narGHIJK should be mostly predicted by 
## distance, and norBC, napAB and nrfAH should all be better 
## predicted by the abundance of the sox operon.

def regrShallowNgeneAgainstSox(Ngene):
    shallowCounts = shallowNenzT[Ngene].reset_index(drop=True)
    shallowDist = shallowNenzT.reset_index()['Distance']
    sox = shallowSenzT['soxYZAXB'].reset_index(drop=True)
    model = sm.GLM(
                        shallowCounts, 
                        sox, 
                        family=sm.families.NegativeBinomial())
    model_results = model.fit()
    return(model_results.summary())

regrShallowNgeneAgainstSox("napAB")

regrShallowNgeneAgainstSox("nirBD")

regrShallowNgeneAgainstSox("nrfAH")

## none of this is convincing. I'm missing something here, I think
## I'm not 
## Let's look at some plots:
## https://medium.com/analytics-vidhya/linear-regression-using-pandas-numpy-for-beginners-in-data-science-fe57157ed93d

## according to our shallow and deep RDAs, 
## seems like the N genes most likely correlated with the sox operon are:

genes = [
    "nrfAH",
    "napAB",
    "nirK",
    "nirS", ##
    "norBC",
        ]


aa = (pd.concat([shallowNenzT[genes], shallowSenzT["soxYZAXB"]], axis="columns")
        .reset_index())
aa = aa[['soxYZAXB', "nirS", 'nrfAH', 'napAB', 'nirK', 'norBC', 'Distance']]
shallowCorrs = aa.corr()
sns.pairplot(data=aa)
plt.gcf().suptitle('Shallow nitrogen/SOX operon correlations', fontsize=20)

## after relocating image:
plt.gcf().tight_layout()

shallowCorrs

## that makes sense.

## make some models out of these
## uncouple the R diagrams of RDAs

## repeat with the deep sediments.
## the 


bb = deepNenzT[genes]
bb = (pd.concat([deepSenzT["soxYZAXB"], bb], axis="columns")
        .reset_index())
bb = bb[['soxYZAXB', 'nrfAH', 'napAB', 'nirK', 'norBC','Distance' ]] 
deepCorrs = bb.corr()
sns.pairplot(data=bb)
plt.gcf().suptitle('Deep nitrogen/SOX operon correlations', fontsize=20)

plt.gcf().tight_layout()

deepCorrs

shallowCorrs

## okay, we gotta stop procrastinating and make some sort of
## model for these. 

## following:
https://timeseriesreasoning.com/contents/negative-binomial-regression-model/
## which is a damn fine website that I should just study for 
## a couple months
## but life is too short/busy.

## according to this fellow, we need two things to build
## a good negative binomial model from our data. 

## 1)  α, which is a coefficient that estimates our variance
##     from our mean
## 2)  λ, which is the background rate constant that 


## we need to estimate both, then build a model of our 
## so before we build our real model, we do some other
## models to get these parameters

import pandas as pd
from patsy import dmatrices
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf

## start with, for example norBC
expr = """ norBC ~ soxYZAXB + Distance """

## use patsy to make experimental matrix (df), with intercept
y, x = dmatrices(expr, aa, return_type='dataframe')

## we start by trying to estimate λ, using a poisson regression

poisson = sm.GLM(y, x, family=sm.families.Poisson()).fit()

print(poisson.summary())

## our rates for λ are here, one per observation
print(poisson.mu)

## store it
aa['LAMBDA'] = poisson.mu

## now we need α

## we estimate with a quick OLS regression of the variance,
## solved for α
## we'll use the second order model of the variance (NB2),
## just because this is what they do in the example.
## really, I have no fucking clue. 
## this part is over my head, I do not understand it totally. 

## we apply the variance formula to all values of lambda
## and run this auxillary model to fit a coefficient to it
## using a quick-and-dirty ols

aa['AUX_OLS_DEP'] = aa.apply(lambda x: ((x['norBC'] - x['LAMBDA'])**2 - x['LAMBDA']) / x['LAMBDA'], axis=1)

## that's weird looking. Let's see if it works. 

## make a patsy expression for this:

ols_expr = """AUX_OLS_DEP ~ LAMBDA - 1"""

aux_olsr_results = smf.ols(ols_expr, aa).fit()

aux_olsr_results.params[0] ## this is our α value. very small

aux_olsr_results.tvalues ## = 0.996688, p=.39, not statistically significant, could probably just use poisson regression?
## oh well, go with it. Probably better than nothing.

nb2_results = sm.GLM(y, x, family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()

print(nb2_results.summary())


####### zhe meeting ########

## it looks like from Zhe's last slide that 
## different organisms are contributing 
## differentially with space (downstream)
## to narG

## give Zhe an example of searchable genome

## finish individual models

## try a mixed model (hierarchical) of 
## all nitrogen genes against sox operon

## water chemisty 


###### mixed model of all nitrogen reduction genes predicted SOX operon ####



data = sm.datasets.get_rdataset("dietox", "geepack").data
md = smf.mixedlm("Weight ~ Time", data, groups=data["Pig"])
mdf = md.fit()

print(mdf.summary())


## I don't understand, why are pvalues always so low?
## not complaining, but...am I using this correctly?

## make a random example 

import random
j = range(90)
obs = [ random.random() for i in j ]
grp = ['zoop']*30 + ['poop']*30 + ['moop']*30
predictor = [ random.random() for i in j ]
randData = pd.DataFrame({'obs':obs,'grp':grp,'predictor':predictor})
randMod = smf.mixedlm("obs ~ predictor", randData, groups=randData["grp"])
randModFit = randMod.fit()
print(randModFit.summary())

## yeah, okay, so you can break the pvalues.

## comforting. move on to our data

## for the moment, ignore the count data aspect and format Zhe's 
## gene counts

## start with the nitrogen reduction genes that 
## are suggested by our RDA, and the sox operon

## not sure if it is meaningful to include the 
## narg operon, the RDA suggests no effect by 
## SOX operon
 
## also, should we combine shallow and deep here?

## let's try both together, without considering 
## depth. Seems like the simplest solution.

## we can break it up later or consider depth as 
## a random effect in the model later

## so our genes of interest are:
genes = ['soxYZAXB', 'nrfAH', 'napAB', 'nirK', 'norBC']
## remake our all-enzyme matrix
#enz = pd.read_excel("Normalized_OnlyEnzymes.xlsx")
enz = pd.read_csv("Normalized_OnlyEnzymes.csv") ## for old pandas on personal machine
enz.rename(columns = {'Unnamed: 0':'enzyme'}, inplace='True')
enz['enzyme'] = enz['enzyme'].str.replace(',','')
enz.set_index('enzyme', inplace=True)
SNenz = enz
aa = (SNenz.reset_index()['enzyme']
        .str.split(pat="(", n=1, expand=True)[1]
        .str.replace(")","")
    )
aa.name = "enzyme"
SNenz.index = aa
SNenz = SNenz.loc[genes].drop(['Function','KU_5'], axis='columns')
SNenz= SNenz.transpose()

SNenz.head()

## push this out for R:
SNenz.to_csv('SNenz.csv')

## what does this look like, with the combined depths,
## not taking distance into account?
 
sns.pairplot(data=SNenz)

## interesting. And can we build one large scatter, 
## colored by type of Ngene?

plt.close('all')

fig, ax = plt.subplots()

for i in SNenz[['nrfAH', 'napAB', 'nirK', 'norBC']].iteritems():
    ax.scatter(SNenz['soxYZAXB'], i[1], label=i[0])
handle, label = ax.get_legend_handles_labels()
ax.legend(handle, label)
ax.set_title('Selected N-reduction genes against SOX operon abundance, both depths', fontsize=20)
ax.set_ylabel('N-reducing Genes', fontsize=15)
ax.set_xlabel('soxYZAXB', fontsize=15)

ax.set_aspect('equal', 'box')


## so, to build a model? We need to melt and rebuild with enzyme as a categorical variable

df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
                   'B': {0: 1, 1: 3, 2: 5},
                   'C': {0: 2, 1: 4, 2: 6}})

pd.melt(df, id_vars=['A'], value_vars=['B'])

df

pd.melt(SNenz, id_vars=[''], value_vars=[''])

pd.melt(SNenz, id_vars=[''], value_vars=[''])

SNenz.head()

SNenz.transpose().reset_index()

help(SNenz.reset_index)

SNenz.reset_index()


## make our dataframe for the model:

aa = (SNenz
    .reset_index(drop=False)
    .drop(axis="columns", labels="soxYZAXB")
    .rename(columns={'index':'site'})
)

bb = pd.melt(aa, id_vars='site', value_name='geneCopyCount') 

soxes = pd.concat([SNenz['soxYZAXB'],
                   SNenz['soxYZAXB'],
                   SNenz['soxYZAXB'],
                   SNenz['soxYZAXB']])

bb['soxYZAXB'] = soxes.values

(bb.site == soxes.index).all() ## order looks okay

## write out for R
bb.to_csv('SNenzMelted.csv')

## make a model

help(smf.mixedlm)

soxM = smf.mixedlm("geneCopyCount ~ soxYZAXB", bb, groups=bb["enzyme"])

soxMfit = soxM.fit()

soxMfit.summary()

## somehow we have to fit a better family of errors to this. 
## I hate complex frequentist linear models. All the terminology
## becomes to specialized and complex quickly.

## we made a simple glm  with count data above:
nb2_results = sm.GLM(y, x, family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()





##### make genomes searchable #########

## let's grab one of the genomes that zhe gave us and take a look
## see if we can make a database out of it:


cd /home/daniel/Documents/analyses/zheSpat/refgenomes/ncbi_dataset/data/GCA_002742695.1


makeblastdb -in GCA_002742695.1_ASM274269v1_genomic.fna -parse_seqids -dbtype nucl

## works

## genes of interest are here:

cd /home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxB_SD_5


## huh, this looks like Zhe took the genes from reference genomes and 
## not his own data? not sure. 

## anyway, to look for them in the first refgenome them:
soxGenes="/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxB_SD_5/fasta.fna"
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/ncbi_dataset/data/GCA_002742695.1/GCA_002742695.1_ASM274269v1_genomic.fna"

less $soxGenes


blastn -db $localDB -query $soxGenes > testBlast.txt

blastn -db $localDB -query $soxGenes -out testBlast.txt -perc_identity 50 -outfmt 6 

## and nothing. Why? 

## sample sequence we know is in there:

tail $localDB > test.fasta

blastn -db $localDB -query test.fasta -out testBlast.txt -perc_identity 50 -outfmt 6 

blastn -db $localDB -query test.fasta ## yeah works

head -n 2 $soxGenes

blastn -db $localDB -query <(head -n 2 $soxGenes)

## weird. 

## well, I'm pretty sure the sox operon is in Paracoccus


cd /home/daniel/Documents/analyses/zheSpat/refgenomes/Paraccocus

## combine the genomes

cat Paraccocus_pantotrophus_chromosome2.fasta  Paracoccus_pantotrophus_chromosome1.fasta > Paracoccus_combined.fasta

makeblastdb -in Paracoccus_combined.fasta -parse_seqids -dbtype nucl


soxGenes="/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxB_SD_5/fasta.fna"
## change our db to paracoccus
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/Paraccocus/Paracoccus_combined.fasta"

blastn -db $localDB -query $soxGenes -out testBlast.txt 

blastn -db $localDB -query $soxGenes -out testBlast.txt -perc_identity 50 -outfmt 10

blastn -db $localDB -query $soxGenes -out testBlast.csv -outfmt 10
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' testBlast.csv

## we're going to have to fix the names of the queries


sed 's/3300045504 assembled //g' $soxGenes > refgenomes/lookingForGenes/soxB_SD_5/fastaCleanedHeaders.fna

soxBgenes=refgenomes/lookingForGenes/soxB_SD_5/fastaCleanedHeaders.fna

blastn -db $localDB -query $soxBgenes -out paracoccusSoxBtestBlast.csv -outfmt 10
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' paracoccusSoxBtestBlast.csv

blastn -db $localDB -query $soxBgenes > paracoccusSoxBtestBlast_longForm.txt  

## and a manual blast on the ncbi webapp of their whole nucleotide dbase 
## yields lot of hits, some pretty close

## saved as: Ga0495066_0870750_2_412_allNCBIblastmatches.txt

## so these genes are out there. just not in the Sulfuricurvum genome?

## try this with the new genome that Zhe sent. Apparently the soxB genes are present 
## in some Sulfuricurvum strains and not others??

## try the other genome that zhe sent
## try the other sox genes that zhe sent
## try the "confirmed" S. kujiensis soxB gene that zhe sent

#### look in s. kujinse GCF_000183725.1 for soxB

cd /home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCF_000183725.1/

makeblastdb -in GCF_000183725.1_ASM18372v1_genomic.fna -parse_seqids -dbtype nucl

## change our db to S. kujinse GCF_000183725.1
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCF_000183725.1/GCF_000183725.1_ASM18372v1_genomic.fna"

soxBgenes="/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxB_SD_5/fasta.fna"

blastn -db $localDB -query $soxBgenes -out GCF_000183725.1_SoxB_Blast.csv -outfmt 10
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' GCF_000183725.1_SoxB_Blast.csv
blastn -db $localDB -query $soxBgenes > GCF_000183725.1_SoxB_Blast_longForm.txt  

## nope, nada

## try the "known" gene that Zhe sent, that should be in S.kuj genomes

kujSoxB="/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/SoxB_S.kujiense.fasta"

#localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCA_002742695.1/GCA_002742695.1_ASM274269v1_genomic.fna"
#genome="GCA_002742695.1"

localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCF_000183725.1/GCF_000183725.1_ASM18372v1_genomic.fna"
genome="GCF_000183725.1"

blastn -db $localDB -query $kujSoxB -out $genome"_SoxB_Blast.csv" -outfmt 10

blastn -db $localDB -query $kujSoxB 

## okay, it's in the GCF_000183725.1 genome all the way
## but don't find it in the GCA genome. Maybe if we lower the percent ID?

blastn -db $localDB -query $kujSoxB -perc_identity 50 | less

## nope. weird.
## the gca genome is a MAG, anyway, not sure if it is relevant: 
## https://www.ncbi.nlm.nih.gov/bioproject/PRJNA391989/
## from this paper
## https://pubmed.ncbi.nlm.nih.gov/28484436/




## for sanity checks, let's get the skuji genome that Zhe found this soxB gene, directly from ncbi

cd /home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/DSM16994/
makeblastdb -in sulfuricurvum_kujiense_DSM16994_genome.fasta -parse_seqids -dbtype nucl

localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/DSM16994/sulfuricurvum_kujiense_DSM16994_genome.fasta"
genome="DSM16994"
blastn -db $localDB -query $kujSoxB -out $genome"_SoxB_Blast.csv" -outfmt 10
blastn -db $localDB -query $kujSoxB > $genome"_SoxB_Blast_longForm.txt"  
## yes, that works, hundred percent full match as expected
## not sure why I can't find this soxB gene in the other sulfuricurvum genomes. or our version from zhe's data.

## let's see if we can find the soxB genes from Zhe's data in this new skurv genome:
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/DSM16994/sulfuricurvum_kujiense_DSM16994_genome.fasta"
genome="DSM16994"
blastn -db $localDB -query $soxBgenes -out $genome"_SoxB_Blast.csv" -outfmt 10


## nope. ab Odd. 

## try the paracoccus genome, with this new soxB from skuj:

localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/Paraccocus/Paracoccus_combined.fasta"
genome="Paracoccus"
blastn -db $localDB -query $kujSoxB > $genome"_kujSoxB_Blast_longForm.txt"  
## nope. nada

less $kujSoxB 

## let's try a abbreviated sequence:
shortKujSoxB="TGAGGGCGAAGCTTTGGTTAAAGAGCTCTATTCACCATATGATTCCGAACTTTCCGAAGTTTTGGGAATTACTAAAAATACGCTGTTTAAACGCGACACGTTCCATTCAACGTTCGATCAGTTGATCAATGACTCTATTATCGATGCGAT"

## paracoccus
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/Paraccocus/Paracoccus_combined.fasta"
genome="Paracoccus"
query="shortKujSoxB"
blastn -db $localDB -query <(echo $shortKujSoxB) > $genome"_"$query"_longForm.txt"  
## nope. 

## other non DSM skuj genomes with short kujsoxB

## GCF_000183725.1
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCF_000183725.1/GCF_000183725.1_ASM18372v1_genomic.fna"
genome="GCF_000183725.1"
query="shortKujSoxB"
#blastn -db $localDB -query <(echo $shortKujSoxB) > $genome"_"$query"_longForm.txt"  
blastn -db $localDB -query <(echo $shortKujSoxB) 
## ah, there it is. Same problem with the other non-DSM genome?




## so what if we really relax the perc_identity cutoff? Not sure what the default is:

localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/GCF_000183725.1/GCF_000183725.1_ASM18372v1_genomic.fna"
genome="GCF_000183725.1"
query="kujSoxB"

blastn -db $localDB -query $kujSoxB | less

blastn -db $localDB -query $kujSoxB -perc_identity 50 | less

## sanity check
localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/DSM16994/sulfuricurvum_kujiense_DSM16994_genome.fasta"
genome="DSM16994"
query="shortKujSoxB"
blastn -db $localDB -query <(echo $shortKujSoxB) > $genome"_"$query"_longForm.txt"  
##  works, of course. 

## and can we find the SOXb

## to do - combine all of Zhe's genes, run blast against all three skuj genomes

## let's get all of Zhe's sox genes together, including soxB

## we need to use tblastn for these faa files, I think?


soxB='/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxB.fna'
query="soxB"

soxA='/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxA.faa'
query="soxA"

soxX='/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxX.faa'
query="soxX"

soxY='/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxY.faa'
query="soxY"

soxZ='/home/daniel/Documents/analyses/zheSpat/refgenomes/lookingForGenes/soxZ.faa'
query="soxZ"

localDB="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/data/DSM16994/sulfuricurvum_kujiense_DSM16994_genome.fasta"
genome="DSM16994"

blastn -db $localDB -query $soxB -out $genome"_"$query"_Blastn.csv" -outfmt 10  
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_Blastn.csv"

tblastn -db $localDB -query $soxA | less

tblastn -db $localDB -query $soxA  -out $genome"_"$query"_tBlast.csv" -outfmt 10 
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_tBlast.csv"
## no strong matches.

tblastn -db $localDB -query $soxX  -out $genome"_"$query"_tBlast.csv" -outfmt 10 
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_tBlast.csv"
## no strong matches.

tblastn -db $localDB -query $soxY  -out $genome"_"$query"_tBlast.csv" -outfmt 10 
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_tBlast.csv"

tblastn -db $localDB -query $soxZ  -out $genome"_"$query"_tBlast.csv" -outfmt 10 
sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_tBlast.csv"

## I'm just not seeing a lot here...

## a genome of interest is the colorado sulfuricurvum genome, because of the coupled sulfur/nitrate metabolism

### to do ###

## find the mags
## search Zhe's mags for the Nreducing and Soxidizing genes
## build mixed model
## rebuild denbi login
## find Zhe's assembly, check it for SOX 


#### mixed model with count data with neg binomial or poisson #####

## a good example here: 
## https://pages.stat.wisc.edu/~bates/UseR2008/WorkshopD.pdf

## let's try lme4, with the glmm function 

install.packages('nloptr')
install.packages('lme4')

library('lme4')

SNenz <- read.csv('SNenz.csv', row.names=1)
SNenzMelted <- read.csv('SNenzMelted.csv', row.names=1)

## example from the package
data('Dyestuff')

fm1 <- lmer(Yield ~ 1 + (1 | Batch), Dyestuff)
print(fm1)

## so for us it would look like this:

SNenzMelted

head(SNenzMelted)

simpModel <- lmer(soxYZAXB ~ 1 + (1 | enzyme) + (geneCopyCount), SNenzMelted)

print(simpModel)

## can test this against an intercept only model:

simpModelInter <- lmer(soxYZAXB ~ 1 + (1 | enzyme), SNenzMelted)

print(simpModelInter)

anova(simpModel, simpModelInter) ## Nreducing gene counts are a statistically significant predictor

## oops, we have that backwards. We've mostly been trying to predict the abundance 
## of the multiple nitrogen reducing genes by the abundance of the sox operon

## flip this in the next model

## so we want to build a model of some denitrifying genes by SOX operon counts:

soxByNgenesModels <-  glmer(geneCopyCount ~ 1 + (1 | enzyme) + (soxYZAXB), 
                      data = SNenzMelted, 
                      family = "poisson")

print(soxByNgenesModels)


exp(0.0006313)

## lots of warnings for being non-integers. 
## which raises some interesting questions. 

## I've assumed that the normalized abundances
## (which are not integers) still hold all the
## other properties of count data. 

## I really think they do. We could multiply 
## all values by ten or something...

## anyway, something to think about.

## for now, carry on, intercept only model:

soxByNgenesModels <-  glmer(geneCopyCount ~ 1 + (1 | enzyme), 
                      data = SNenzMelted, 
                      family = "poisson")

anova(simpModel, simpModelInter) 

## again, sox gene counts are a highly significant predictor
## of the nitrogen reducing gene abundances
## AIC values drop for the full model, also

## log into denbi


#### Zhe meeting Jul 13 #####

## Zhe needs a phylogenetic tree of MAGs
## for example, fig.3 or 
## https://www.nature.com/articles/s41467-021-27769-5.pdf
## need genes, maybe ribosomal proteins
## how to extract ribosomal protein seqs
## is this the best gene for this?

## look sulfuricurvum soxB gene in assembly

## Zhe's mags and such are on emik in:

cd '/emic/Zhe/Assembly_MAGs'

## we should grab this folder locally. It's not that big. 

tar cvzf zheAssembliesMAGs.tar.gz Assembly_MAGs

ssh -Y -i /home/daniel/.ssh/ssh_emic emic@134.176.27.78 -p 30022

## get this, from back on home computer

scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    emic@134.176.27.78:/emic/Zhe/zheAssembliesMAGs.tar.gz \
    /home/daniel/Documents/analyses/zheSpat/bigFiles \
    &> scplog.txt & 

## big, 8.2 gig. Why?
## to start, we have a few assenbly files that are bigger than 1 gig. 

find . -type f -size +1G -exec ls {} -lh \;



tar -xvf zheAssembliesMAGs.tar.gz

## first task to is search these for the sulfuricurvum 
## SOX gene

## Zhe was interested in the soxB gene, maybe we look for the 
## others also

## the kujiense genome with annotations is here:
https://www.ncbi.nlm.nih.gov/nuccore/CP002355.1
## this has numerous sox gene protein sequences  

## made an faa file from the genbank annotated genome,
## that has five sox genes in it:

kujiSoxes="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/kujiensis_allSox.faa"

## put it on emik
scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    $kujiSoxes \
    emic@134.176.27.78:/home/emic/test/ 


## looks like diamond doesn't handle nucleotide 
## databases.
## we have small data anyway, let's use good old blast?

## make a searchable database of Zhe's assembly:

ls /emic/Zhe/Assembly_MAGs/Assembly/

## there are eight files in here, and they are kind of 
## big. 

## cycle through them:

cd /emic/Zhe/Assembly_MAGs/Assembly/


cd /emic/Zhe/Assembly_MAGs/Assembly
kujiSoxes="/home/emic/test/kujiensis_allSox.faa"

for i in *; do
  echo "starting "$i
  fastaFileName=${i%.gz}
  ## unzip
  gunzip $i
  ## make db
  echo "making blastdb"
  makeblastdb -in $fastaFileName -parse_seqids -dbtype nucl
  ## do searches, write out results long and short form
  ## short
  echo "running blast searches"
  tblastn -db $fastaFileName  \
      -query $kujiSoxes \
      -out ${fastaFileName%.fasta}"_kujiSoxes_tBlast.csv" \
      -outfmt 10 \
      -num_threads 16
  sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' ${fastaFileName%.fasta}"_kujiSoxes_tBlast.csv"
  ## long form
  tblastn -db $fastaFileName  \
      -query $kujiSoxes \
      -out ${fastaFileName%.fasta}"_kujiSoxes_tBlast.txt" \
      -num_threads 16
  echo "cleaning up, rezipping"
  ## clean up indexing files
  rm $fastaFileName.n*
  ## zip file back up
  gzip $fastaFileName
  echo "done with "$fastaFileName
  unset i
  unset fastaFileName
done

## actually looks like that worked. 

## to get it locally:

scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    emic@134.176.27.78:/emic/Zhe/Assembly_MAGs/Assembly/blastSearches/kujiSoxAss.tar.gz .

## if I find an interesting result, how do I get it out of the
## the alignment?

## for example, in S02 contigs, NODE_3791787_length_268_cov_0.901408
## matches pretty strongly to SoxY from Sulfuricurvum

cd ~/Documents/analyses/zheSpat/bigFiles/Assembly_MAGs/Assembly

ls S02_contigs.fasta

NODE_3791787_length_268_cov_0.901408

grep NODE_3791787_length_268_cov_0.901408 S02_contigs.fasta -A 1

## i know there must be one or more good command one-liners
## for doing extracting individual sequences but I can't 
## remember them. Use biopython

python3

import Bio, os

from Bio import SeqIO

for seq_record in SeqIO.parse("S02_contigs.fasta", "fasta"):
    print(seq_record.id)
    print(repr(seq_record.seq))
    print(len(seq_record))


aa = SeqIO.parse("S02_contigs.fasta", "fasta")

    print(seq_record.id)

[print(seq_record.seq) for seq_record in SeqIO.parse("S02_contigs.fasta", "fasta") ]

[print(seq_record.seq) for seq_record in SeqIO.parse("S02_contigs.fasta", "fasta") if seq_record.id == "NODE_3791787_length_268_cov_0.901408" ]

## gives us
CGGTAATTTGTATGCTGCAGATGGAAGACTTTATGCGGCGTATGAAAAAACGCAATTACTGGATGCTTCGGGCAAGCCCATCAAAGCATCCATGCTTGAGCAAGAGGTGAATTATATTTTCAACTATCCGTATGCATCTACACCGTGTATATTGGTCAATTTGCCCAAGCCTACACAGCAGGATGTAGAATTAACTGCAGAGAATGGTGAAAAATACGTGTGGAAAAGCGGTGTCGGGAAAAATCGTACGATTGTGGCGTATGTGGCT

## which gives us no reverse blast match on the ncbi data base.
## odd. 

## interesting, maybe look more later. 

## of most interest is the soxB gene

## one of our better matches for S02 :

[
    print(seq_record.seq) for seq_record 
    in SeqIO.parse("S02_contigs.fasta", "fasta") if 
    seq_record.id == "NODE_6152945_length_241_cov_1.032258"
]

## yields a match to Sulfuriferula genome, but not the kujiensis soxB

## check other assemblies 

## great, so the plan would be to 
## examine all strong matches to sulfuricurvum sox genes
## especially sox B

## so what would be the best way to search our mags for 
## something related to sox genes from sulfuricurvum?

## I want separate searches for each of the sulfuricurvum sox genes
## this was a mess before, all together

## so split up the sox query file we used before into 5 files, loop through them:


## broken up queries are here:
soxDir="/home/daniel/Documents/analyses/zheSpat/refgenomes/sulfuricurvum/soxIndividual"

## locally the mags are here:
magDir="/home/daniel/Documents/analyses/zheSpat/bigFiles/Assembly_MAGs/MAGs/S_15"

## we need searchable MAG databases:

cd $magDir
for i in *.fa; do 
    echo $i
    makeblastdb -in $i -parse_seqids -dbtype nucl
done

for i in $magDir/*.fa; do 
echo $i
done

## put our mag sulf sox search results here?
magKujiSox="/home/daniel/Documents/analyses/zheSpat/kujiSoxQueries/magkujiSox"

cd $magDir
for i in *.fa; do 
echo $i
done

for sox_i in $soxDir/*; do 
echo $sox_i
done

cd $magDir
for sox_i in $soxDir/*; do 
    base_sox_i=$(basename $sox_i)
    sox_i_name=${base_sox_i%.faa}
    letter=${sox_i_name#kujiensis_sox}
    for i in *.fa; do 
      fileoutName=$magKujiSox"/"${i%.fa}"_kujiSox"$letter"_tBlast"
      ## short form
      tblastn -db $i  \
          -query $sox_i \
          -out $fileoutName".csv" \
          -outfmt 10 \
          -num_threads 5
      ## long form
      tblastn -db $i  \
          -query $sox_i \
          -out $fileoutName".txt" \
          -num_threads 5
    done
done


## to just get top hits to sulfuricurvum soxB among our MAGs:

cd /home/daniel/Documents/analyses/zheSpat/kujiSoxQueries/magkujiSox


head -n 1 -q *SoxB*csv > allMagSoxB_best.txt

## better ways to do this, but we can find our way back with this:
head -n 1 *SoxB*csv > allMagSoxB_best_fileinfo.txt

## what is our best match, overall, according to bp match?
cut -d , -f 3 allMagSoxB_best.txt | sort  
## 64.706, corresponds to SPAdesHybrid-SD.17
## but actually a really crappy match

## what is our best match, overall, according to bitscore?
cut -d , -f 12 allMagSoxB_best.txt | sort   ## 

## because of formatting, escore is harder to parse
cut -d , -f 2,11 allMagSoxB_best.txt | grep "e-" | wc -l 

cut -d , -f 2,11 allMagSoxB_best.txt | grep "e-" 

## if use just those matches that are < 10^2 we are down to six matches:

cut -d , -f 2,11 allMagSoxB_best.txt | grep "e-[0-9]\{3\}" | wc -l

cut -d , -f 2,11 allMagSoxB_best.txt | grep "e-[0-9]\{3\}" 

cut -d , -f 2,11 allMagSoxB_best_fileinfo.txt | grep -B 1 "e-[0-9]\{3\}" | grep "==>" > kujiSoxBstrongMatchMAGs.txt

## these are:

SPAdesHybrid-SA.35
SPAdesHybrid-SD.4
SPAdes-S04.16
SPAdes-S04.4
SPAdes-S06.11
SPAdes-S06.6

## to look at these matches:

less SPAdesHybrid-SA.35_kujiSoxB_tBlast.txt 
less SPAdesHybrid-SD.4_kujiSoxB_tBlast.txt 
less SPAdes-S04.16_kujiSoxB_tBlast.txt 
less SPAdes-S04.4_kujiSoxB_tBlast.txt 
less SPAdes-S06.11_kujiSoxB_tBlast.txt ## this one seems particularly convincing
less SPAdes-S06.6_kujiSoxB_tBlast.txt  ## also not bad

## can we back blast on of these?

## SPAdes-S06.11_kujiSoxB_tBlast.txt 
DLLKFDATGNVTLLHVADIHGQLMPIYFREPSVNLGVGEARGQPPHLTGKDFLKRFGIPE
KSASAYALTDQGFEALAKSYGRIGGLDRLATVVKRVRAERGDDKVLFLDGGDTWQGSLGA
NRSKGQDMVDCMALLKPDAMTGHWEFTYGEARVNELIKQLGFPFLALNIRDTEWN
EPAFEPMKLFEKGGVKIAVLGQAFPYTPVANPRWLMPKWSFGIREEDVRANVEKARKA-G
AQLVVLLSHNGFDVDRKLATRVKGIDVILTGHTHDALPEAIKVGQTLLVASGSHGKFISR
LDLDIRDGAVKGFRYKLIPLFADVIKPDAEMTAAVTKARAPFARELSRVVGHAESLLYRR
GNFNGTFDDLICATLLKERDAEIALSPGFRWGTSVLPGQAITVEDIHNATAITYPQAYRI
SMTGERLKEVLEDVADNLFNPDPYYQQGGDMVRCGGLGYAINIVKPIGQRISAMTQLKSG
KPIEPKKEYVVTGWASVNEGTEGPPVWDLVERYVAAEKTVRIAPNASVKI






cut -d , -f 2,11 allMagSoxB_best.txt | grep "e-[0-9]\{2\}" 

## 30 of our MAGs have a low enough escore that they need a negative exponent.


## just according to the escores, some candidates for 
## mags holding some version of the sulfuricurvum SoxB
## gene would be:


SPAdesHybrid-SA.104 ## acetobacteria, 80%, 1.4 contam
SPAdesHybrid-SA.32 ## proteobacteria, probably delta, 72%, 2.0 contam 
 ## this genome is really similar to Sub.37 (96.4% ANI) 
SPAdesHybrid-SA.35 ## proteobacteria, 55%, 5% contam 
SPAdesHybrid-SA.38 ## firmicutus, 70%, 1.4% contam
SPAdesHybrid-SA.59 ## deltaproteo, 53% complete, 14% contam 
SPAdesHybrid-SA.62 ## Firmicutus, Planococcocaeae, 73%, 5% contam
SPAdesHybrid-SA.79 ## Candidate div, Zixibacteria, 67%, 0% contam 




## soxY
head -n 1 -q *SoxY*csv > allMagSoxY_best.txt

head -n 1 *SoxY*csv > allMagSoxY_best_fileinfo.txt

## meeting Jul 20 ##

## can I find a mag that has a good match to all sulfuricurvum SOX genes
## dive into this later.



################################

### let's start working on Zhe's phylo tree
## seems like he'll probably use SingleM to get the ribosomal proteins out
## then we should make a tree, phyml or something
## the output should a newick, therefore

## so maybe we start from a newick file, work on the upstream
## steps if Zhe wants us

## we really want a circular tree. let's use d3js...

##############

cd /emic/Zhe/Assembly_MAGs/Assembly/

conda activate pplacerEnv

singlem pipe --sequences SUB_contigs.fasta.gz --otu_table otu_table.csv --threads 24

############

## get zhe a fasta file for each ribosomal protein,
## with all the mags that have that sequence data for that
##  ribosomal protein

## let's get it locally:

scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    emic@134.176.27.78:/emic/Zhe/Assembly_MAGs/MAGs/SingleM/otu_MAGs_diamond_example.csv .

head otu_MAGs_diamond_example.csv
## make a copy to work with to avoid destroying data
cp otu_MAGs_diamond_example.csv ribProts.csv

S1.10.ribosomal_protein_S7  SPAdesHybrid-SD.18

## we only need the first three columns

## labels
paste <(cut -f 1 ribProts.csv ) <(cut -f 2 ribProts.csv) -d ':' | tail -n +2 > labels.txt
sed -i 's/^/>/' labels.txt
## sequences
cut -f 3 ribProts.csv | tail -n +2 > sequences.txt 
paste -d '\n' labels.txt sequences.txt > zheAllMagsAllRibProts.fasta

## but downstream programs are detecting duplicate headers. see below.


## while we're at it, let's make a set of the protein names:
cut -f 1 ribProts.csv | tail -n +2  | sort -u > ribProtList.txt

## great. let's use biopython to separate by protein?

python3

from Bio import SeqIO
import pandas as pd
import re

##fastaFolder='/home/daniel/Documents/analyses/zheSpat/ribProtFastas/'
fastaFolder='/home/daniel/Documents/analyses/ZheStreamData/ribProtFastas/'

## looks like there are sometimes multiple versions of each ribosomoal 
## protein sequence from the same mags... this is a problem.
## some of these are triplicate, even. 
## shoot.
## so how to add this information into our labels?

with open('ribProtList.txt', 'r') as list:
    for i in list:
        i = i.strip()
        print(i)
        fileName = (fastaFolder + i + ".fasta")
        recList = []
        dupList = []
        for seq_record in SeqIO.parse("zheAllMagsAllRibProts.fasta", "fasta"):
           if seq_record.id.__contains__(i):
                print(seq_record.id)
                dupList.append(seq_record.id)
                ## get the duplicate number, if any
                matches = (pd.Series(dupList) == seq_record.id).sum()
                if matches == 1: repNu = None
                elif matches > 1 : 
                    repNu = matches
                    seq_record.id = (seq_record.id + "_" + str(repNu))
                print(seq_record.id)
                ## proceed with fasta assembly
                seq_record.description, seq_record.name = '',''
                recList.append(seq_record)
        SeqIO.write(recList, fileName, 'fasta')



## let's see if that worked, below with modeltest-ng:

################################################################################
## ## original 
## with open('ribProtList.txt', 'r') as list:
##     for i in list:
##         print('')
##         print(i.strip())
##         fileName = (fastaFolder + i.strip() + ".fasta")
##         recList = []
##         for seq_record in SeqIO.parse("zheAllMagsAllRibProts.fasta", "fasta"):
##              if seq_record.id.__contains__(i.strip()):
##                 seq_record.description, seq_record.name = '',''
##                 recList.append(seq_record)
##         SeqIO.write(recList, fileName, 'fasta')
################################################################################

## put back onto denbi

scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    -r ./ribProtFastas/ \
    emic@134.176.27.78:/emic/Zhe/Assembly_MAGs/MAGs/ 

## so on denbi the files are here:
cd /emic/Zhe/Assembly_MAGs/MAGs/ribProtFastas

## a little worried, because Zhe said there is missing data,
## but I do not see anything missing... hmm...

## let's put it up there and see if Zhe can do a few sanity checks on it.

## I think the steps would be something like:

## align individual genes
## run evolutionary model
## concatenate
## partitioning
## tree construction

## alignments should be simple, 

## to do one:
ribProtFolder="/home/daniel/Documents/analyses/ZheStreamData/ribProtFastas/"
ribProtFasta_i="S1.10.ribosomal_protein_S7.fasta"
ribProt_i=$(basename $ribProtFasta_i .fasta)
outAlignFolder="/home/daniel/Documents/analyses/ZheStreamData/RPtree/alignments/"
alignOutFasta_i=$ribProt_i"_aligned.fasta"
alignOutLog_i=$ribProt_i"_alignment.log"

## check these...
echo $ribProtFolder
echo $ribProtFasta_i
echo $ribProt_i
echo $outAlignFolder
echo $alignOutFasta_i
echo $alignOutLog_i


muscle -in $ribProtFolder$ribProtFasta_i -log $outAlignFolder$alignOutLog_i -out $outAlignFolder$alignOutFasta_i


## to do them all:
ribProtFolder="/home/daniel/Documents/analyses/ZheStreamData/ribProtFastas/"
outAlignFolder="/home/daniel/Documents/analyses/ZheStreamData/RPtree/alignments/"

for ribProtFasta_i in $ribProtFolder*; do
    ribProt_i=$(basename $ribProtFasta_i .fasta)
    alignOutFasta_i=$ribProt_i"_aligned.fasta"
    alignOutLog_i=$ribProt_i"_alignment.log"
    muscle -in $ribProtFasta_i -log $outAlignFolder$alignOutLog_i -out $outAlignFolder$alignOutFasta_i
    echo $ribProt_i
done

## next we need an evolutionary model for them
## to be totally correct, we should even check these 
## individual genes for differential rates of evolution,
## make partitions within them.
## for now let's assume each gene has its own model, across the 

## whole sequence, and not try to find the partitions within. 

## I will leave this up to Zhe, if he wants we can pursue 
## partitions

## for finding the models
## looks like the tool of interest would be:
https://github.com/ddarriba/modeltest

## they have a 64-bit linux binary here:

wget https://github.com/ddarriba/modeltest/files/6192913/modeltest-ng-0.1.7-static.tar.gz

## dependencies:

sudo apt-get install flex bison

## they have binaries for linux, try it out




ls $alignmentFolder
ls $alignment_i

## to find the evolution model for one alignment:

evoModelFolder="/home/daniel/Documents/analyses/ZheStreamData/RPtree/evoModels/"
alignmentFolder="/home/daniel/Documents/analyses/ZheStreamData/RPtree/alignments/"
alignment_i="S1.10.ribosomal_protein_S7_aligned.fasta"

ls $alignmentFolder$alignment_i

modeltest-ng --help

modeltest-ng \
    -i $alignmentFolder$alignment_i \
    -d nt \
    -o $evoModelFolder"test.fasta" \
    -p 3 \
    -v &> testlog.txt

## for all alignments:
for i in $alignmentFolder/*.fasta; do
    j=$(basename $i)
    ls $alignmentFolder$j 
    echo $evoModelFolder$j
    modeltest-ng \
        -i $alignmentFolder$j \
        -d nt \
        -o ${j/_aligned.fasta/_evoModel} \
        -p 3 \
        -T phyml ## since I like phyml. can change
done

#####
## but model test is detecting duplicate headers. For instance,
## S1.10.ribosomal_protein_S7:SPAdesHybrid-SD.18:

grep "S1.10.ribosomal_protein_S7:SPAdesHybrid-SD.18" zheAllMagsAllRibProts.fasta -A 1

## these are not the same sequence, so we can't throw them out. 

## what do we do with these?

## Zhe's problem. For the moment, give them names that reflect their duplicate label status 

## okay, fixed in the individual fastas that are divided up by gene:

ribProtFolder="/home/daniel/Documents/analyses/ZheStreamData/ribProtFastas/"
grep "S1.10.ribosomal_protein_S7:SPAdesHybrid-SD.18" -R -A 1  $ribProtFolder

## repeat the above alignment 

## great, seems to be working. How do we use this output? and how do 
## concatenate a matrix for phyml?

## first, can we install modeltest-ng on de.nbi for zhe?

denbiDinosaur

conda install modeltest-ng

conda create -n modeltest-ng -c bioconda modeltest-ng

## looks great.

## now, how do we concatenate?

## let's try FASconCAT:
https://github.com/PatrickKueck/FASconCAT-G



cd RPtree/alignments


FASconCAT-G_v1.05.1.pl -o 

FASconCAT-G_v1.05.1.pl

## this seems to work, in a directory of fasta alignment files:

FASconCAT-G_v1.05.1.pl -o -o -p -s

## not sure if this what we want, but this returns no errors

phyml -i FcC_supermatrix.phy -b 100

## fails. It looks to me like the alignments are being kept separate, not 
## concatenated

## as in, there are ~850 lines in this supermatrix. 

## let's simplify, and try just two files:

cd RPtree/toyRibProtAlignments

FASconCAT-G_v1.05.1.pl -s
## yeah, it just sort of interleaves the files. 
## I guess  this makes sense, the original alignments 
## are not the same size, due to missing data
## how could we expect FASconCAT to know what to line up to to what?
## so, do we need to rename these to have the same fasta record headers,
## so the software knows what to concat?
## the fasconcat doesn't say much about how it creates rows, 
## whether by order or by seq ID...


alignDir="/home/daniel/Documents/analyses/zheSpat/RPtree/alignments/"
toyDir="/home/daniel/Documents/analyses/zheSpat/RPtree/toyRibProtAlignments/"

cd $toyDir
##head -n 12 $alignDir"S1.10.ribosomal_protein_S7_aligned.fasta" > alignment1.fasta
##head -n 12 $alignDir"S1.1.ribosomal_protein_L2_rplB_aligned.fasta"  > alignment2.fasta

cp $alignDir"S1.10.ribosomal_protein_S7_aligned.fasta" alignment1.fasta
cp $alignDir"S1.1.ribosomal_protein_L2_rplB_aligned.fasta" alignment2.fasta

rm FcC_*

cat alignment1.fasta

sed -i "s/>.*:/\>/" alignment1.fasta
sed -i "s/>.*:/\>/" alignment2.fasta

FASconCAT-G_v1.05.1.pl -s

## that looks better...

## try that on all the alignments:

cp -r $alignDir supMatrix/

cd RPtree/supMatrix

sed -i "s/>.*:/\>/"  *

#FASconCAT-G_v1.05.1.pl -s

FASconCAT-G_v1.05.1.pl -o -o -p -p -s ## to get phylip format, not strict, for phyml

## let's put this on the de.nbi

scp -P 30022 \
    -i /home/daniel/.ssh/ssh_emic \
    /home/daniel/Documents/analyses/zheSpat/RPtree/supMatrix/FcC_supermatrix.phy \
    emic@134.176.27.78:/emic/Zhe/danAdditions/RPtree


## on emic

cd /emic/Zhe/danAdditions/RPtree


phyml FcC_supermatrix.phy 

## as far as evo models, not sure how to vary these among the 
## genes. I think we need make a partition file, and 
## not sure if phyml accepts this.

## the deafult model for phyml, HKY is a good model for most of 
## of the genes, but not S1.10.ribosomal_protein_S7. 

## TPM1uf+G4 also seems to work most but not S1.14.ribosomal_protein_S19_rpsS
## TPM3 is in all somewhere, + uf+G4 is in most. 

## not sure how to get these more complex models...

## for the moment, let's run HKY alone on the supermatrix, for simplicity

## here is the template:
phyml [ sequences data_type format data_sets bootstrap_sets model
        [kappa] invar nb_categ alpha tree opt_topology opt_lengths ]


phyml FcC_supermatrix.phy 0 s 1 100 HKY e e 1 e BIONJ y y

## can we do this in a way that we don't need to be on the computer?

## how about a script?

################
#!/usr/bin/env bash

cd /emic/Zhe/danAdditions/RPtree
phyml FcC_supermatrix.phy 0 s 1 100 HKY e e 1 e BIONJ y y 2>&1> phymlLog080822.txt

###############

chmod 777 danPhyml080822.sh


ssh -p 30022 -i /home/daniel/.ssh/ssh_emic emic@134.176.27.78 bash danPhyml080822.sh

## that worked, looks like ~4 hours on denbi

## get tree file local, try it out on our existing d3 code:


## zhe meeting 

## 
## add column of csv file with information about duplicates, sorted 
## once we decide about the duplicates, rerun the tree-building pipeline
## install fasConcat on emik
  

#### 1. add dup information to zhe's spreadsheet

## zhe's spreadsheet of the ribosomal proteins is here:

/home/daniel/Documents/analyses/zheSpat/otu_MAGs_diamond_example.csv

## we're going to want python for this:

python3

from Bio import SeqIO
import pandas as pd
import os

zheData = pd.read_csv("otu_MAGs_diamond_example.csv", sep="\t")


fastaFolder='/home/daniel/Documents/analyses/zheSpat/ribProtFastas/'
##fastaFolder='/home/daniel/Documents/analyses/ZheStreamData/ribProtFastas/'
os.listdir(fastaFolder)

## looks like there are sometimes multiple versions of each ribosomoal 
## protein sequence from the same mags... this is a problem.
## some of these are triplicate, even. 
## shoot.
## so how to add this information into our labels?

masterDupList=[]
with open('ribProtList.txt', 'r') as list:
    for i in list:
        i = i.strip()
        print(i)
        dupList = []
        nameList = []
        for seq_record in SeqIO.parse("zheAllMagsAllRibProts.fasta", "fasta"):
           if seq_record.id.__contains__(i):
                print(seq_record.id)
                dupList.append(seq_record.id)
                ## get the duplicate number, if any
                matches = (pd.Series(dupList) == seq_record.id).sum()
                nameList.append(seq_record.id + ":" + str(matches))
        masterDupList += nameList


masterDupSeries = pd.Series(masterDupList)
dupData = masterDupSeries.str.split(':', expand=True)
dupData.columns = ['gene','sample','dup']
dupData['dup'] = dupData['dup'].astype('int')

## can we merge on two columns?

aa = pd.merge(zheData, dupData, on=['gene','sample'])

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(dupData)

aa.duplicated().any()

## ah, so the join is causing extra rows to be created.

## maybe better to sort them, and concatenate.

dupData.columns = ['gene','sample','dup']

zheData_sorted = zheData.sort_values(['gene','sample']).reset_index(drop=True)
dupData_sorted = dupData.sort_values(['gene','sample']).reset_index(drop=True)


cc = pd.concat([zheData_sorted,dupData_sorted], axis='columns')
cc.columns = ['gene', 'sample', 'sequence', 'num_hits', 'coverage', 'taxonomy', 'dupgene', 'dupsample', 'dupNum']
## sanity check
(cc['gene'] == cc['dupgene']).all()
(cc['sample'] == cc['dupsample']).all()
## looks good. 
## drop extra columns
cc.drop(columns=['dupgene', 'dupsample'], inplace=True)


with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(cc)

cc.to_csv('dupData.csv', index=False)
